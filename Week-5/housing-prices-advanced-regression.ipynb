{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data 605 Final Exam: Problem 2\n### David Blumenstiel"},{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"cell_type":"code","source":"#lets get this one out of the way first\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(MASS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5 points.  Descriptive and Inferential Statistics. Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any three quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?**"},{"metadata":{},"cell_type":"markdown","source":"First thing we need to do is load in the data.  We'll reserve the testing set for later, and just load in the training set as 'df' for now.  Let's also take a look at the shape of the data along with univariate descriptive statistics for each variable.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df <- read.csv('../input/house-prices-advanced-regression-techniques/train.csv')\n\ndim(df)\nsummary(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, there are quite a few variables; 81 of them to be exact.  There are 1460 observations in this training set, which should be ample for our purposes.\n\nI'm not going to go into the details of every variable above, but you can see there are a few different types of data.  We have categorical data, continuous numeric data, and discrete numerical data (some of which would be better off as categorical data).  There's some stuff like 'ID' that has little bearing, and missing data which could be coerced to more appropriate values.  \n\nLet's take a more in depth look at the response variable, 'SalePrice'.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(df$SalePrice)\nhist(df$SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As one might expect, prices are somewhat right skewed, with most houses being worth around $130,000 to 214,000 with some houses worth significantly more.\n\nNow we'll do a scatterplot/correlation matrix for a few of the independent variables and the dependent variable.  We'll choose \"SalePrice\",\"LotArea\",\"YearBuilt\",\"FullBath\",\"OpenPorchSF\",\"GrLivArea\",\"TotalBsmtSF\".  These ones have some reasonable correlation to the sale price."},{"metadata":{"trusted":true},"cell_type":"code","source":"colnames(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs.panels(df[,c(\"SalePrice\",\"LotArea\",\"YearBuilt\",\"FullBath\",\"OpenPorchSF\",\"GrLivArea\",\"TotalBsmtSF\")])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above we have the pairs plots containing correlations and scatters.  Some things that stand out are that the above ground living room square area, 'GrLivArea', is the most highly correlated dependent variable of the set to 'SalePrice' with a correlation coefficient of 0.71, and lot area is the least correlated with a coefficient of 0.26.  This makes some sense, as greater living area makes for bigger houses, and smaller lots tend to include things like city homes/apartments, which may be expensive despite having small lots.\n\nSome other interesting facets: the newer the house, the more full-bathrooms and basement space; lot area has little effect on how big the porch gets; the number of full bathrooms are highly correlated to the amount of living space.\n\nFor now, we'll cut this down to three variables as requested: \"SalePrice\",\"FullBath\" and \"GrLivArea\".\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pairs.panels(df[,c(\"SalePrice\",\"FullBath\",\"GrLivArea\")])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll test the hypothesis that the correlations between each pairwise set is 0, and provide an 80% confidence interval."},{"metadata":{"trusted":true},"cell_type":"code","source":"cor.test(df$SalePrice, df$FullBath, conf.level = 0.8)\ncor.test(df$SalePrice, df$GrLivArea, conf.level = 0.8)\ncor.test(df$FullBath, df$GrLivArea, conf.level = 0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above are the three correlation tests, one for each pair.  The hypothesis for all three goes as: if the p-value is less than the significance level, then the correlation is not 0 (there is a correlation).  The p-value for all three pairs is pretty close to 0, so at the standard significance level of 0.05, we can conclude that there is indeed a correlation between each of the variables and one-another individually.  \n\nWhen it comes to the correlation coefficients themselves, all are estimated to be positive, indicating potential positive correlations (if x increases, y increases ) among all pairs.  The 80% confidence intervals can be interpreted as: \"We are 80% confident that the true correlation coefficient falls within this range.\"  If that range excludes 0, we can further be 80% sure that there are indeed correlations here,\n\nFamilywise error here is the likelihood that at least one of correlation findings were false; i.e., there isn't a correlation between one of the pairs.  Given how low the p-values are, and that there are only three correlations, I'm not worried about familywise error.  In fact, we can quantify the familywise error rate as: $FWE \\leq 1 - (1-\\alpha)^c$ where $\\alpha$ is the significance level, and c is the number of tests performed.  Because the p-values are near 0, we could have set the significance level to something stupid like 0.00001, and had a very small familywise error rate.\n\nOn an interesting side note, if you think about how scientific literature tends to go by a 0.05 significance level, you kind of start to get an idea of just how much stuff out there probably isn't right.\n"},{"metadata":{},"cell_type":"markdown","source":"**5 points. Linear Algebra and Correlation.  Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix. **\n\nBelow, we'll give the correlation matrix from before (in actual matrix form this time) along with the precision matrix.  We'll also multiply the two together (both ways)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cormat <- cor(x=df[c(\"SalePrice\",\"FullBath\",\"GrLivArea\")]) #Gotta actually make the correlation matrix as a matrix first\n\ncormat\n\npermat <- solve(cormat) #The percision matrix\n\npermat\n\ncormat_by_permat <- cormat %*% permat #correlation matrix * percision matrix\n\ncormat_by_permat\n\npermat_by_cormat <- permat %*% cormat #percision matrix * correlation matrix\n\npermat_by_cormat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll perform LU decomposition on the correlation matrix.  The 'pracma' package has a nice function for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"#LU Decomposition of the correlation matrix\nlibrary(pracma)\nLUDecomp_cormat <- lu(cormat)\nLUDecomp_cormat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5 points.  Calculus-Based Probability & Statistics.  Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.  Then load the MASS package and run fitdistr to fit an exponential probability density function.  (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of $\\lambda$ for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000,$\\lambda$ )).  Plot a histogram and compare it with a histogram of your original variable.   Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).   Also generate a 95% confidence interval from the empirical data, assuming normality.  Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.**\n\nIf you remember, our response variable, 'SalePrice', is right skewed; we'll use that.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"saleprice <- df$SalePrice #Makes it easier to call\n\nif (min(saleprice) <= 0) { #Checks if the min is 0; it isn't\n    print(\"OHHH NOO!  You had better do somthing!\")\n} \n\n#library(MASS) #Loads the MASS library, did this earlier\n\nprice_expdist <- fitdistr(saleprice, \"exponential\") #Fit's the sale prices to the exponential distribution\n\nrate <- price_expdist$estimate #This is the optimal lambda from above\n\nset.seed(42) #Sets the seed so you see what I see in this next part\n\nexp_samples <- rexp(1000,rate) #Takes 1000 samples from the exponental distribution of sale price\n\npar(mfrow = c(1,2)) #Makes side by side plots\nhist(exp_samples, breaks = 50) #Histogram of the exponential samples\nhist(saleprice, breaks = 50)  #Histogram of the origional data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, they are different.  The exponential samples weigh heavily towards 0, which would be unlikely for housing prices in most situations.  The scales are similar, although the exponential distribution has a much wider spread.\n\nNow let's find the 5th and 95th percentiles of *the* exponential pdf using the cumulative distribution function (using our rate parameter).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"qexp(0.05, rate) #The 5th percentile\nqexp(0.95, rate) #The 95th percentile\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting, but let's also compare it to the quantiles of the numbers we sampled from our exponential distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"quantile(exp_samples, c(0.05, 0.95))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty close.  As we take more samples (samples -> $\\infty$) we should expect the theoretical quantiles to match the actual ones, because the samples here are derived directly from the theoretical distribution.\n\nNow let's find the Now let's find the 95% confidence interval around the mean for our empirical data (from the original dataset).  We're assuming normality, so we should be able to get this from the t.test function.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"t.test(saleprice, conf.level = 0.95)$conf.int","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In other words, we're 95% confident that the mean Sale Price of the population at large is between about $176,842 and 185,000.\n\nNow let's find the empirical 5th and 95th percentiles for our actual data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"quantile(saleprice, c(0.05,0.95))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considering the mean is $180,921, I think we can say that this distribution is right skewed.  But what would the quantiles look like if they were coming from a normal distribution?"},{"metadata":{"trusted":true},"cell_type":"code","source":"qnorm(0.05, mean = mean(saleprice), sd= sd(saleprice))\nqnorm(0.5, mean = mean(saleprice), sd= sd(saleprice))\nqnorm(0.95, mean = mean(saleprice), sd= sd(saleprice))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThose percentiles would about evenly straddle the median, and the mean would be similar to the median.  We can pretty well guess that our 'SalePrice' data isn't coming from a population with a normal distribution for sale price.  Worse yet, however, would be classifying our data as exponential; those statistics are even farther off.\n"},{"metadata":{},"cell_type":"markdown","source":"**10 points.  Modeling.  Build some type of multiple regression  model and submit your model to the competition board.  Provide your complete model summary and results with analysis.  Report your Kaggle.com user name and score.**\n\nThere are of ways we could do this.  I know there are neater, fancier tactics for narrowing down on a good model, but I'm going go with the tried and true \"use it all then delete variables until it looks nice\" method.\n\nFirst though, we need to clean things up a bit.  Lot's of NA's and whatnot in here.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df <- read.csv('../input/house-prices-advanced-regression-techniques/train.csv')\n\n#Going to make a function for this so we can do the same to the testing set\n#Note: this was built with the training and testing sets' NAs in mind.  If one were to use other data, this would need to be more comprehensive in it's handling of NAs and other undesireable cases.\nclean <- function(df){\n\n    df$Id <- NULL # Get rid of the ID\n\n    df$LotFrontage[is.na(df$LotFrontage)]<- 0 #If it doesn't have frontage then set to 0\n\n    levels(df$Alley) <- c(levels(df$Alley), \"None\")  #If no alley then make a new factor level, 'None'\n    df$Alley[is.na(df$Alley)] <- 'None'\n\n    df$Utilities <- NULL  #all but one has utilties\n\n    df$Neighborhood <- NULL #Too many / won't generalize well\n\n    df$HouseStyle <- NULL #Not going to generalize well\n    \n    df$MasVnrType[is.na(df$MasVnrType)] <- \"None\" #If not available then change to none (likely)\n    \n    df$MasVnrArea[is.na(df$MasVnrArea)] <- 0  #Not likely to have any\n \n    levels(df$BsmtQual) <- c(levels(df$BsmtQual), \"None\")\n    df$BsmtQual[is.na(df$BsmtQual)] <- \"None\" #Na is decribed in the metadata as no basement\n    levels(df$BsmtCond) <- c(levels(df$BsmtCond), \"None\")\n    df$BsmtCond[is.na(df$BsmtCond)] <- \"None\" #Na is decribed in the metadata as no basement\n    levels(df$BsmtExposure) <- c(levels(df$BsmtExposure), \"None\")\n    df$BsmtExposure[is.na(df$BsmtExposure)] <- \"None\" #Na is decribed in the metadata as no basement\n    levels(df$BsmtFinType1) <- c(levels(df$BsmtFinType1), \"None\")\n    df$BsmtFinType1[is.na(df$BsmtFinType1)] <- \"None\" #Na is decribed in the metadata as no basement\n    levels(df$BsmtFinType2) <- c(levels(df$BsmtFinType2), \"None\")\n    df$BsmtFinType2[is.na(df$BsmtFinType2)] <- \"None\" #Na is decribed in the metadata as no basement\n    \n    df$Electrical[is.na(df$Electrical)] <- 'SBrkr' #Makes the one instance of this the typical value\n    \n    levels(df$FireplaceQu) <- c(levels(df$FireplaceQu), \"None\") #Na is no fireplace\n    df$FireplaceQu[is.na(df$FireplaceQu)] <- 'None'\n     \n    levels(df$GarageType) <- c(levels(df$GarageType), \"None\") #Na is no garage\n    df$GarageType[is.na(df$GarageType)] <- 'None'\n    df$GarageYrBlt[is.na(df$GarageYrBlt)] <- median(df$GarageYrBlt, na.rm = TRUE) #No great way to do this, but there's only 81 instances of NA's here in the training set\n    levels(df$GarageFinish) <- c(levels(df$GarageFinish), \"None\") #Na is no garage\n    df$GarageFinish[is.na(df$GarageFinish)] <- 'None'\n    levels(df$GarageQual) <- c(levels(df$GarageQual), \"None\") #Na is no garage\n    df$GarageQual[is.na(df$GarageQual)] <- 'None'\n    levels(df$GarageCond) <- c(levels(df$GarageCond), \"None\") #Na is no garage\n    df$GarageCond[is.na(df$GarageCond)] <- 'None'\n    \n    df$PoolArea <- NULL # Are few pools in training data\n    df$PoolQC <- NULL\n    \n    df$MiscFeature <- NULL #Not enough sheds to go around\n    df$MiscVal <- NULL\n    \n    levels(df$Fence) <- c(levels(df$Fence), \"None\") #Na is no fence\n    df$Fence[is.na(df$Fence)] <- 'None'\n    \n    df$MoSold <- as.factor(df$MoSold) #This is supposed to be categorical data\n    \n    df$YrSold <- as.factor(df$YrSold) #Will also work well as categorical data\n    \n    \n    df$MSZoning[is.na(df$MSZoning)] <- 'RL' #Makes instances of this the typical value\n    \n    \n    \n    df$Exterior1st[is.na(df$Exterior1st)] <- 'VinylSd'#Makes instances of these the typical value\n    df$Exterior2nd[is.na(df$Exterior2nd)] <- 'VinylSd'\n    \n    df$BsmtFinSF1[is.na(df$BsmtFinSF1)] <- 0  #These NAs mean no basement, so we'll change the various footages to 0\n    df$BsmtFinSF2[is.na(df$BsmtFinSF2)] <- 0\n    df$BsmtUnfSF[is.na(df$BsmtUnfSF)] <- 0\n    df$TotalBsmtSF[is.na(df$TotalBsmtSF)] <- 0\n    df$BsmtFullBath[is.na(df$BsmtFullBath)] <- 0\n    df$BsmtHalfBath[is.na(df$BsmtHalfBath)] <- 0\n    \n    df$KitchenQual[is.na(df$KitchenQual)] <- 'TA'#Makes instances of these the typical value\n    \n    df$Functional[is.na(df$Functional)] <- 'Typ' #Makes instances of these the typical value\n    \n    df$GarageCars[is.na(df$GarageCars)] <- 0\n    df$GarageArea[is.na(df$GarageArea)] <- 0\n    \n    df$SaleType[is.na(df$SaleType)] <- 'WD' #Makes the one instance of this the typical value\n    \n    \n    return(df) \n}\n\ndf <- clean(df)\n\nsummary(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that's done, we can try out the model that has it all."},{"metadata":{"trusted":true},"cell_type":"code","source":"model <- lm(SalePrice ~., data = df)\nstr(df)\nsummary(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adjusted R-squared of 0.9047; pretty good assuming it's not overfitting, which it may very well be given how many variables we're using.  Ordinarily one could split off a validation set to verify that, but Kaggle let's you submit your answers a bunch of times, so we'll just wait to test it out on the test set.\n\nYou might notice a few NA coefficients to in the summary above.  I believe R is doing this because those terms can be estimated by combinations of other terms.  Notably, NA coefficients tend to occur on the last category of a categorical variable, (e.g. basement condition), which may be summarized by the preceding coefficients from other categories.  However, NA does appear on basement area as well, which is numeric, and I'm not sure why that is; it was found highly predictive in the EDA.\n\nAlso, you'll probably notice there's a lot there that just really doesn't do much for the model.  Now's the part where we clean it up a bit by getting rid of the independent variables that don't do much.  For this, I'm going to employ stepwise regression (a somewhat controversial practice, I've read), which will try to eliminate some of the variables based on a specific criterion.  In this case, we'll use stepAIC, which here will select fewer variables by attempting to minimize the AIC score; in essence, this tries to cut down on variables while keeping the adjusted r-squared similar.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"step <- stepAIC(model, direction=\"backward\", trace=FALSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary(step)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Still not exactly pretty, but that reduced the terms down to 112 while keeping the adjusted r-squared about the same.\n\nLet's take a closer look at how the model behaves"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Starting with residuals vs fitted, we can see that for most values, the residuals are distributed fairly normally.  However, as homes get higher in price, the scale of residuals grows.  I'm thinking this could be due to a relative shortage of homes in this dataset worth over $300,000 compared to cheaper homes; maybe the model just learned how to predict the lower valued homes better.  A small curve in residuals is also indicated, which here means it may be somewhat overvaluing the very cheap homes.  We didn't use any squared terms in the model for the sake of simplicity, but those may have helped.  Overall though, the residuals seem fairly normally distributed.\n\nAs for the Q-Q plot, there's a noticeable 'wiggle' to it.  The heavy tails suggest more extreme than we would normally (pun intended) have.  However, most values still lie on the line.\n\nThe scale location plot doesn't look that great.  That the line curves like that indicated non-linearity in the model, and indicated some heteroskedasticity.\n\nIn the residuals vs leverage plot, we only have two influential cases (the ones behind the dashed lines), but these don't throw off the model too much as indicated by the horizontal red line.\n\nOverall, this tells us that the model could definitely be improved, but should work for our purposes.\n\nNow, let's make our predictions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"testdf <- read.csv('../input/house-prices-advanced-regression-techniques/test.csv') #Reads in the test data\n\ntestID <- testdf$Id #Set's the IDs aside\n\ntestdf <- clean(testdf)  #Cleans it just like the training data\n\nsummary(testdf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions <- predict(model, testdf) #Makes the predictions\n\nsum(is.na(predictions)) #Checks for NAs\n\nhead(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Awesome.  Now we just need to format our predictions and submit.  \n\nThere are definitely better, cleaner models that could be made; this is an example of brute forcing it with most the variables you have at hand.\n\nThis had a root mean squared logarithmic error of about 0.47, so it's not the best model.  I think it overfit quite a bit.  Improvements in variable selection and adding an exponent or two might help improve it.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#formats the data and outputs it to a csv\n\noutput = data.frame(testID, predictions)\ncolnames(output) = c(\"Id\", \"SalePrice\")\nwrite.csv(output, \"Predictions.csv\", row.names=FALSE)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}
